{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "e7zK5ycIHXv_"
      ],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Important Libraries"
      ],
      "metadata": {
        "id": "9dKmfA6Xs6NM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -qq git"
      ],
      "metadata": {
        "id": "bcgHoGboXh8M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cloning the repo\n",
        "!git clone https://github.com/mimisheen/test.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGGuqn6oXtR0",
        "outputId": "7abf2e5c-64e3-4731-f47f-ed6d1a257986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'test'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (6/6), 19.96 KiB | 9.98 MiB/s, done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output\n",
        "!wget https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml\n",
        "!pip install opencv-contrib-python==4.5.5.64\n",
        "!pip install pydub\n",
        "!pip install -U openai-whisper\n",
        "!apt-get install ffmpeg\n",
        "!pip install gtts\n",
        "!apt-get update && apt-get install -y libportaudio2\n",
        "!pip install --upgrade sounddevice\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "yeLCRolQ2QeQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from IPython.display import display, Javascript, HTML\n",
        "from google.colab.output import eval_js\n",
        "import base64\n",
        "from base64 import b64decode\n",
        "from PIL import Image\n",
        "import os, sys, cv2\n",
        "import io\n",
        "from io import BytesIO\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from google.colab.patches import cv2_imshow\n",
        "from gtts import gTTS\n",
        "import sounddevice as sd\n",
        "import soundfile as sf\n",
        "from gtts.tokenizer import pre_processors, tokenizer_cases, Tokenizer\n",
        "import whisper"
      ],
      "metadata": {
        "id": "cnX4sbzz1Sz1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Required to mount drive to save images and audio files.\n",
        "'''\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "'''"
      ],
      "metadata": {
        "id": "lVXz3ykQEsVG",
        "outputId": "6ff8f27b-04e5-4424-cae4-e0e8847781c0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Use this command if you have your own database of images and audio files.\n",
        "#!cp -r /content/drive/MyDrive/<YOUR_database_folder_name> /content/database"
      ],
      "metadata": {
        "id": "dlZ9KRyBskb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main UI for video, audio and transcription (Javascript)"
      ],
      "metadata": {
        "id": "iRA8X5AjG5KR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the html css code integrated in google colab environment. You will notice html css is integrated in javascript code. This is because you cannot directly access your local webcam or microphone in google colab environment."
      ],
      "metadata": {
        "id": "Y-G3REjwdrCO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HTML and JavaScript code to access webcam, capture photo, and record audio\n",
        "html_code = \"\"\"\n",
        "<!DOCTYPE html>\n",
        "<html lang=\"en\">\n",
        "<head>\n",
        "  <meta charset=\"UTF-8\">\n",
        "  <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
        "  <title>Webcam and Audio Recorder</title>\n",
        "  <style>\n",
        "    body {\n",
        "      font-family: Arial, sans-serif;\n",
        "      margin: 0;\n",
        "      padding: 20px;\n",
        "      background-color: #f4f4f4;\n",
        "    }\n",
        "    .container {\n",
        "      max-width: 800px;\n",
        "      margin: 0 auto;\n",
        "      padding: 20px;\n",
        "      background-color: white;\n",
        "      border-radius: 10px;\n",
        "      box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);\n",
        "    }\n",
        "    .video-container {\n",
        "      position: relative;\n",
        "      width: 100%;\n",
        "      height: 480px;\n",
        "      background-color: black;\n",
        "      margin-bottom: 20px;\n",
        "      display: flex;\n",
        "      justify-content: center;\n",
        "      align-items: center;\n",
        "    }\n",
        "\n",
        "    video {\n",
        "      width: 100%;\n",
        "      height: 100%;\n",
        "      object-fit: cover;\n",
        "      background-color: black;\n",
        "      z-index: 1;\n",
        "    }\n",
        "\n",
        "    canvas {\n",
        "      position: absolute;\n",
        "      top: 0;\n",
        "      left: 0;\n",
        "      width: 100%;\n",
        "      height: 100%;\n",
        "      z-index: 2;\n",
        "      pointer-events: none;\n",
        "    }\n",
        "\n",
        "    #personNameContainer {\n",
        "      display: none;\n",
        "    }\n",
        "    textarea {\n",
        "      position: relative;\n",
        "      width: 100%;\n",
        "      height: 100px;\n",
        "      padding: 10px;\n",
        "      border-radius: 5px;\n",
        "      border: 1px solid #ccc;\n",
        "      font-size: 16px;\n",
        "      resize: none;\n",
        "      box-sizing: border-box;\n",
        "    }\n",
        "    #status {\n",
        "      margin-top: 10px;\n",
        "    }\n",
        "    .name-tag {\n",
        "        padding: 10px 20px;\n",
        "        background-color: #4caf50;\n",
        "        color: white;\n",
        "        border-radius: 5px;\n",
        "        font-size: 16px;\n",
        "        font-weight: bold;\n",
        "    }\n",
        "  </style>\n",
        "</head>\n",
        "<body>\n",
        "  <div class=\"container\">\n",
        "    <h1>Webcam Capture and Audio Recorder</h1>\n",
        "\n",
        "    <!-- Video element for webcam feed -->\n",
        "    <video id=\"webcamVideo\" autoplay playsinline muted></video>\n",
        "    <img id=\"overlayImg\" style=\"position: absolute; top: 0; left: 0;\" />\n",
        "    <div id=\"labelDisplay\" style=\"position: absolute; top: 10px; left: 10px; color: white; background: rgba(0,0,0,0.5); padding: 5px;\"></div>\n",
        "    <div id=\"errorMsg\"></div>\n",
        "    <div id=\"debugOutput\"></div>\n",
        "    <div id=\"statusElement\"></div>\n",
        "\n",
        "    <!-- Input for person's name (Initially hidden) -->\n",
        "    <div id=\"personNameContainer\">\n",
        "      <h3>Enter Person's Name:</h3>\n",
        "      <input type=\"text\" id=\"personName\" placeholder=\"Enter Name\" />\n",
        "      <br><br>\n",
        "      <button onclick=\"startCapture()\">Start Capturing</button>\n",
        "    </div>\n",
        "\n",
        "    <canvas id=\"captureCanvas\"></canvas>\n",
        "\n",
        "    <!-- Buttons for Webcam -->\n",
        "    <button id=\"startWebcamBtn\" onclick=\"startWebcam()\">Start Webcam</button>\n",
        "    <button id=\"stopWebcamBtn\" onclick=\"stopWebcam()\">Stop Webcam</button>\n",
        "    <button id=\"captureBtn\" onclick=\"showNameInput()\">Capture Photo</button>\n",
        "\n",
        "    <!-- Audio Recorder Buttons -->\n",
        "    <h3>Audio Recorder</h3>\n",
        "    <button id=\"startAudioBtn\">Start Recording</button>\n",
        "    <button id=\"stopAudioBtn\">Stop Recording</button>\n",
        "    <p id=\"audioStatus\"></p>\n",
        "\n",
        "    <p id=\"status\"></p>\n",
        "\n",
        "    <!-- Text box -->\n",
        "    <h3 style=\"display: flex; align-items: center; gap: 10px;\">\n",
        "      Transcription:\n",
        "      <span id=\"detectedLanguageContainer\" style=\"font-size: 14px; color: #555; padding: 5px 10px; background-color: #f0f0f0; border-radius: 5px;\">\n",
        "        Detected Language: <span id=\"detectedLanguageBox\">None</span>\n",
        "      </span>\n",
        "    </h3>\n",
        "    <textarea id=\"transcriptionBox\" placeholder=\"Transcription will appear here...\"></textarea>\n",
        "    <!-- Save Text Button -->\n",
        "    <button id=\"saveTextBtn\" onclick=\"saveText()\">Save Text</button>\n",
        "  </div>\n",
        "\n",
        "  <script>\n",
        "    // Variable Declarations (Global Scope)\n",
        "    let stream = null;\n",
        "    let captureCount = 0;\n",
        "    const maxCaptures = 50; // Set to 50 captures\n",
        "    const captureInterval = 2000; // 2000 ms (2 seconds)\n",
        "    let frameCount = 0;\n",
        "    let modelNames = {};\n",
        "    let frameCounter = 0; // Global frame counter\n",
        "    let audioStream = null;\n",
        "    const videoElement = document.getElementById('webcamVideo');\n",
        "    const captureCanvas = document.getElementById(\"captureCanvas\");\n",
        "    const statusElement = document.getElementById('statusElement'); // Properly initialized once\n",
        "    const personNameInput = document.getElementById('personName');\n",
        "    const personNameContainer = document.getElementById('personNameContainer');\n",
        "    const captureBtn = document.getElementById('captureBtn');\n",
        "    const audioStatusElement = document.getElementById('audioStatus');\n",
        "\n",
        "    let mediaRecorder, audioRecorder, chunks = [], audioChunks = [], capturing = false;\n",
        "\n",
        "    // Function to Start Webcam\n",
        "    function startWebcam() {\n",
        "        if (navigator.mediaDevices.getUserMedia) {\n",
        "            navigator.mediaDevices.getUserMedia({ video: true })\n",
        "                .then(function(localStream) {\n",
        "                    stream = localStream;\n",
        "                    videoElement.srcObject = stream;\n",
        "                    statusElement.innerText = \"Webcam is running...\";\n",
        "\n",
        "                    videoElement.addEventListener(\"loadedmetadata\", () => {\n",
        "                        // Start capturing frames\n",
        "\n",
        "                        captureFrames();\n",
        "                    });\n",
        "                })\n",
        "                .catch(function(err) {\n",
        "                    console.log(\"Error: \" + err);\n",
        "                    alert(\"Unable to access webcam: \" + err);\n",
        "                });\n",
        "        } else {\n",
        "            alert('getUserMedia is not supported in this browser.');\n",
        "        }\n",
        "    }\n",
        "\n",
        "    // Function to Capture Frames from Webcam\n",
        "    async function captureFrames() {\n",
        "        const captureCanvas = document.createElement(\"canvas\");\n",
        "        const ctx = captureCanvas.getContext(\"2d\");\n",
        "\n",
        "        captureCanvas.width = videoElement.videoWidth;\n",
        "        captureCanvas.height = videoElement.videoHeight;\n",
        "\n",
        "        const processDuration = 5000; // 5 seconds in milliseconds\n",
        "        let startTime = Date.now(); // Record the start time for processing\n",
        "\n",
        "        async function processFrame() {\n",
        "            const currentTime = Date.now();\n",
        "            if (currentTime - startTime > processDuration) {\n",
        "                console.log(\"Finished processing 5 seconds of frames.\");\n",
        "                //statusElement.innerText = \"Processing complete.\";\n",
        "                return; // Stop processing after 5 seconds\n",
        "            }\n",
        "\n",
        "            // Draw the video frame to the canvas\n",
        "            ctx.drawImage(videoElement, 0, 0, videoElement.videoWidth, videoElement.videoHeight);\n",
        "            const imageDataUrl = captureCanvas.toDataURL(\"image/jpeg\", 0.8);\n",
        "\n",
        "            try {\n",
        "                // Send the captured frame to Python for processing via pred_name() function\n",
        "                await google.colab.kernel.invokeFunction(\"pred\", [imageDataUrl],{});\n",
        "\n",
        "            } catch (error) {\n",
        "                console.error(\"Error calling Python function:\", error);\n",
        "                statusElement.innerText = \"Error calling prediction function.\";\n",
        "            }\n",
        "\n",
        "            // Request the next animation frame to keep processing\n",
        "            requestAnimationFrame(processFrame);\n",
        "        }\n",
        "\n",
        "        processFrame(); // Start the frame processing loop\n",
        "    }\n",
        "\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////////////////////\n",
        "// Function to stop the webcam\n",
        "    function stopWebcam() {\n",
        "      if (stream) {\n",
        "        stream.getTracks().forEach(track => track.stop());\n",
        "        videoElement.srcObject = null;\n",
        "        statusElement.innerText = \"Webcam stopped.\";\n",
        "      }\n",
        "    }\n",
        "\n",
        "////////////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "    async function captureImage(personName) {\n",
        "      const video = document.getElementById(\"webcamVideo\");\n",
        "      const canvas = document.createElement(\"canvas\");\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      const ctx = canvas.getContext(\"2d\");\n",
        "      ctx.drawImage(video, 0, 0, canvas.width, canvas.height);\n",
        "      const imageDataUrl = canvas.toDataURL(\"image/jpeg\", 0.8);\n",
        "\n",
        "      // Call Python function to save and process image\n",
        "      google.colab.kernel.invokeFunction(\"save_and_process_image\", [imageDataUrl, captureCount, personName], {});\n",
        "    }\n",
        "////////////////////////////////////////////////////////////////////////////////////////\n",
        "    // Function to show the name input field and Capture button\n",
        "    function showNameInput() {\n",
        "      captureBtn.style.display = 'none';  // Hide Capture button\n",
        "      personNameContainer.style.display = 'block';  // Show name input field\n",
        "    }\n",
        "////////////////////////////////////////////////////////////////////////////////////////\n",
        "    function startCapture() {\n",
        "      const personName = personNameInput.value.trim();\n",
        "      if (personName === \"\") {\n",
        "        alert(\"Please enter the person's name.\");\n",
        "        return;\n",
        "      }\n",
        "      captureCount = 0;\n",
        "\n",
        "      const intervalId = setInterval(() => {\n",
        "        if (captureCount >= maxCaptures) {\n",
        "          clearInterval(intervalId);\n",
        "          document.getElementById(\"status\").innerText = \"Capture complete.\";\n",
        "          // Begin training after capturing all images\n",
        "          google.colab.kernel.invokeFunction(\"train_model\", [], {}).then(() => {\n",
        "            document.getElementById(\"status\").innerText = \"Training completed.\";\n",
        "          }).catch((error) => {\n",
        "            document.getElementById(\"status\").innerText = \"Training failed.\";\n",
        "            console.error(error);\n",
        "          });\n",
        "        } else {\n",
        "          document.getElementById(\"status\").innerText = `Capturing... ${captureCount}/50`;\n",
        "          captureImage(personName);\n",
        "          captureCount++;\n",
        "        }\n",
        "      }, captureInterval);\n",
        "    }\n",
        "////////////////////////////////////////////////////////////////////////////////////////\n",
        "\n",
        "    // Audio Recorder JavaScript\n",
        "    document.getElementById('startAudioBtn').addEventListener('click', async () => {\n",
        "      audioStream = await navigator.mediaDevices.getUserMedia({ audio: true });\n",
        "      audioRecorder = new MediaRecorder(audioStream);\n",
        "      audioChunks = [];\n",
        "\n",
        "      audioRecorder.ondataavailable = event => {\n",
        "        audioChunks.push(event.data);\n",
        "      };\n",
        "\n",
        "      audioRecorder.onstop = async () => {\n",
        "        const blob = new Blob(audioChunks, { type: 'audio/wav' });\n",
        "        const reader = new FileReader();\n",
        "        reader.onloadend = function() {\n",
        "          const base64data = reader.result.split(',')[1];\n",
        "          google.colab.kernel.invokeFunction('notebook.save_audio', [base64data, 0], {});\n",
        "        };\n",
        "        reader.readAsDataURL(blob);\n",
        "        audioStatusElement.innerText = \"Audio recording stopped. Audio saved.\";\n",
        "      };\n",
        "\n",
        "      audioRecorder.start();\n",
        "      audioStatusElement.innerText = \"Recording audio...\";\n",
        "    });\n",
        "\n",
        "    document.getElementById('stopAudioBtn').addEventListener('click', () => {\n",
        "      audioRecorder.stop();\n",
        "      audioStream.getTracks().forEach(track => track.stop());\n",
        "    });\n",
        "\n",
        "      // Function to save the transcription text when user clicks \"Save Text\"\n",
        "    function saveText() {\n",
        "      const transcriptionText = document.getElementById('transcriptionBox').value;\n",
        "\n",
        "      // Call the Python function to save the text\n",
        "      google.colab.kernel.invokeFunction(\n",
        "        'notebook.save_text',  // Callback registered in Python\n",
        "        [transcriptionText],  // Arguments passed to Python function\n",
        "        {}  // Optional callbacks\n",
        "      ).then(() => {\n",
        "        // Alert user and clear the text box\n",
        "        alert(\"Text saved successfully.\");\n",
        "        document.getElementById('transcriptionBox').value = '';  // Clear the text box\n",
        "      }).catch((error) => {\n",
        "        alert(\"Error saving text: \" + error);\n",
        "      });\n",
        "    }\n",
        "  </script>\n",
        "\n",
        "</body>\n",
        "</html>\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "biM2YteuGDE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the python backend code that is linked with javascript code."
      ],
      "metadata": {
        "id": "DqE2MPsdeg2-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Image processing (Face recognition)"
      ],
      "metadata": {
        "id": "L8GlCT1FrMYQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to decode the image from base64 and save both original and processed images\n",
        "def save_and_process_image(data_url, img_count, person_name):\n",
        "    # Create the person's directory only if it doesn't exist\n",
        "    save_path = Path(\"database\") / person_name\n",
        "    if not save_path.exists():\n",
        "        save_path.mkdir(parents=True, exist_ok=True)\n",
        "        print(f\"Directory created for {person_name} at {save_path}\")\n",
        "\n",
        "    # Decode base64 image\n",
        "    header, encoded = data_url.split(\",\", 1)\n",
        "    binary_data = base64.b64decode(encoded)\n",
        "    img = Image.open(BytesIO(binary_data))\n",
        "\n",
        "    # Convert to OpenCV format\n",
        "    img_cv = cv2.cvtColor(np.array(img), cv2.COLOR_RGB2BGR)\n",
        "\n",
        "    # Convert image to grayscale for face detection\n",
        "    gray = cv2.cvtColor(img_cv, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Load the Haar cascade classifier for face detection\n",
        "    haar_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "    faces = haar_cascade.detectMultiScale(gray, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "    # Draw bounding boxes around faces in the original image\n",
        "    for (x, y, w, h) in faces:\n",
        "        cv2.rectangle(img_cv, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
        "\n",
        "    # Save the processed image with bounding boxes\n",
        "    processed_filename = save_path / f\"processed_{img_count}.jpg\"\n",
        "    cv2.imwrite(str(processed_filename), img_cv)\n",
        "    #print(f\"Processed image with bounding boxes saved as {processed_filename}\")"
      ],
      "metadata": {
        "id": "ZtyeSpXjpYF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training"
      ],
      "metadata": {
        "id": "h4WohQoJFW5g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model():\n",
        "    try:\n",
        "        # Path to the face detection cascade\n",
        "        haar_file = cv2.data.haarcascades + 'haarcascade_frontalface_default.xml'  # Ensure correct path\n",
        "\n",
        "        # Path to the database folder containing subdirectories of face images\n",
        "        datasets = 'database'  # Replace with your actual dataset path\n",
        "        print('Training...')\n",
        "\n",
        "        # Initialize lists to store images, labels, and names\n",
        "        images, labels, names = [], [], {}\n",
        "        id = 0\n",
        "\n",
        "        # Walk through the dataset directory and collect images and corresponding labels\n",
        "        for subdirs, dirs, files in os.walk(datasets):\n",
        "            for subdir in dirs:\n",
        "                if subdir != '.ipynb_checkpoints':  # Exclude notebook checkpoint folders\n",
        "                    print(f\"Processing directory: {subdir}\")\n",
        "                    names[id] = subdir  # Assign a name to the ID\n",
        "                    subjectpath = os.path.join(datasets, subdir)\n",
        "                    for filename in os.listdir(subjectpath):\n",
        "                        path = os.path.join(subjectpath, filename)\n",
        "\n",
        "                        # Read image and convert to grayscale\n",
        "                        img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
        "                        if img is not None and filename.lower() != '.ipynb_checkpoints':\n",
        "                            # Resize the image to a fixed size\n",
        "                            img_resized = cv2.resize(img, (130, 100))\n",
        "                            images.append(img_resized)  # Append resized image\n",
        "                            labels.append(id)  # Append corresponding label\n",
        "                        else:\n",
        "                            print(f\"Skipping file: {filename}\")\n",
        "                    id += 1\n",
        "\n",
        "        # Ensure we have images for training\n",
        "        if len(images) == 0:\n",
        "            raise Exception(\"No images found for training.\")\n",
        "\n",
        "        # Convert images and labels lists to NumPy arrays for model training\n",
        "        images = np.array(images)\n",
        "        labels = np.array(labels)\n",
        "\n",
        "        # Create and train the face recognizer model\n",
        "        model = cv2.face.LBPHFaceRecognizer_create()\n",
        "        model.train(images, labels)\n",
        "        # Save the model to a file after training\n",
        "        model.save('face_recognizer.yml')\n",
        "\n",
        "        # Print training information\n",
        "        print(f\"Training complete with {len(names)} faces.\")\n",
        "        print(f\"Trained on: {names}\")\n",
        "\n",
        "        # Return the trained model and the names dictionary\n",
        "        return model, names\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Training failed with error: {e}\")\n",
        "        return None, None\n",
        "\n"
      ],
      "metadata": {
        "id": "3-VofzMKFQw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prediction"
      ],
      "metadata": {
        "id": "BQHLBLB2lJp7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model, names =train_model()\n",
        "import cv2\n",
        "import numpy as np\n",
        "import base64\n",
        "\n",
        "# Example of how to load a model (make sure the model is properly trained before)\n",
        "model = cv2.face.LBPHFaceRecognizer_create()\n",
        "model.read(\"face_recognizer.yml\")  # Load your trained model\n",
        "\n",
        "def predict(image_data):\n",
        "    try:\n",
        "        # Decode Base64 (handling the comma in the base64 string)\n",
        "        if \",\" in image_data:\n",
        "            base64_str = image_data.split(\",\")[1]\n",
        "        else:\n",
        "            raise ValueError(\"Invalid image data format\")\n",
        "\n",
        "        # Decode base64 string to numpy array\n",
        "        nparr = np.frombuffer(base64.b64decode(base64_str), np.uint8)\n",
        "\n",
        "        # Decode the numpy array to an image\n",
        "        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
        "        if img is None:\n",
        "            raise ValueError(\"Decoded image is empty.\")\n",
        "\n",
        "        # Convert the image to grayscale for face detection\n",
        "        gray_image = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # Load the Haar cascade for face detection\n",
        "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
        "\n",
        "        # Detect faces in the image\n",
        "        faces = face_cascade.detectMultiScale(gray_image, scaleFactor=1.1, minNeighbors=5, minSize=(30, 30))\n",
        "\n",
        "        # Initialize list for detected labels\n",
        "        detected_labels = []\n",
        "\n",
        "        # Process each detected face\n",
        "        for (x, y, w, h) in faces:\n",
        "            # Extract the face region\n",
        "            face = gray_image[y:y+h, x:x+w]\n",
        "\n",
        "            # Resize the face to the required size for the model\n",
        "            resized_face = cv2.resize(face, (150, 150))\n",
        "\n",
        "            # Perform prediction using the trained model\n",
        "            label_id, confidence = model.predict(resized_face)\n",
        "\n",
        "            # Retrieve the name corresponding to the label_id\n",
        "            name = names.get(label_id, \"Unknown\")  # Default to \"Unknown\" if label_id not found\n",
        "\n",
        "            # Append the detected name to the list\n",
        "            detected_labels.append(name)\n",
        "\n",
        "        # If no faces detected, add \"unknown\" to the labels\n",
        "        if not detected_labels:\n",
        "            detected_labels.append(\"unknown\")\n",
        "\n",
        "        # Return the detected labels\n",
        "        return detected_labels\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error in processing image: {e}\")\n",
        "        return []\n",
        "\n",
        "# Example of how you can call the predict function and get the detected names\n",
        "def pred(image_data):\n",
        "    # Call the predict function and get the detected names\n",
        "    detected_names = predict(image_data)\n",
        "\n",
        "    # Print or process the detected names outside the predict function\n",
        "    if detected_names:\n",
        "        print(\"Detected names:\", \", \".join(detected_names))\n",
        "    else:\n",
        "        print(\"No faces detected or error occurred.\")\n",
        "\n",
        "    # Generate JavaScript to update the status element in the browser with the names\n",
        "    detected_names_str = \", \".join(detected_names)\n",
        "    js_code = f\"\"\"\n",
        "    var statusElement = document.getElementById('statusElement');\n",
        "    if (statusElement) {{\n",
        "        statusElement.innerText = 'Detected: {detected_names_str}';\n",
        "    }} else {{\n",
        "        console.error(\"Element with id 'statusElement' not found.\");\n",
        "    }}\n",
        "    \"\"\"\n",
        "    display(Javascript(js_code))\n"
      ],
      "metadata": {
        "id": "gZypwULwlK4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Face recognition evaluation"
      ],
      "metadata": {
        "id": "V3DZJ6pn-bkv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opencv-python\n",
        "!pip install numpy\n",
        "!pip install scikit-learn\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import base64\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
      ],
      "metadata": {
        "id": "pXa1Ie-Sfpss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate the face recognition system\n",
        "def evaluate_face_recognition(model, names, test_images, test_labels):\n",
        "    predictions = []\n",
        "\n",
        "    for image_data, true_label in zip(test_images, test_labels):\n",
        "        # Read the image from the path\n",
        "        image = cv2.imread(image_data, cv2.IMREAD_COLOR)\n",
        "        # Encode the image to base64\n",
        "        retval, buffer = cv2.imencode('.jpg', image)\n",
        "        image_data = base64.b64encode(buffer).decode('utf-8')\n",
        "\n",
        "        result = predict(image_data) # Pass only image_data to predict\n",
        "\n",
        "        if result['status'] == 'success' and len(result['names']) > 0:\n",
        "            predicted_name = result['names'][0]\n",
        "        else:\n",
        "            predicted_name = \"Unknown\"\n",
        "\n",
        "        predicted_label = [label for label, name in names.items() if name == predicted_name]\n",
        "        if predicted_label:\n",
        "            predictions.append(predicted_label[0])\n",
        "        else:\n",
        "            predictions.append(-1)  # Unknown faces\n",
        "\n",
        "    accuracy = accuracy_score(test_labels, predictions)\n",
        "    print(f\"Accuracy: {accuracy * 100:.2f}%\")\n",
        "\n",
        "    precision = precision_score(test_labels, predictions, average='weighted', zero_division=0)\n",
        "    recall = recall_score(test_labels, predictions, average='weighted', zero_division=0)\n",
        "    f1 = f1_score(test_labels, predictions, average='weighted', zero_division=0)\n",
        "\n",
        "    print(f\"Precision: {precision:.2f}\")\n",
        "    print(f\"Recall: {recall:.2f}\")\n",
        "    print(f\"F1 Score: {f1:.2f}\")\n",
        "\n",
        "'''\n",
        "Here mention the file paths of the test images and their corresponding labels.\n",
        "\n",
        "# Example test data (replace with actual base64 images and corresponding labels)\n",
        "test_images = [\n",
        "    \"/content/database/image1.jpg\",\n",
        "    \"/content/database/image2.jpg\",\n",
        "    \"/content/database/image3.jpg\",\n",
        "    \"/content/database/image4.jpg\",\n",
        "    \"/content/database/image5.jpg\",\n",
        "    \"/content/database/image6.jpg\",\n",
        "    \"/content/database/image7.jpg\",\n",
        "    \"/content/database/image8.jpg\",\n",
        "    \"/content/database/image9.jpg\",\n",
        "    \"/content/database/image10.jpg\",\n",
        "]'''\n",
        "\n",
        "test_labels = [0]  # Replace with the actual label IDs corresponding to test images\n",
        "\n",
        "# Train the model\n",
        "model, names = train_model()\n",
        "\n",
        "# Evaluate the model on test data\n",
        "if model and names:\n",
        "    evaluate_face_recognition(model, names, test_images, test_labels)"
      ],
      "metadata": {
        "id": "uFxNlf3qcvX4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are calculating the evaluation metrics for model accuracy in terms of face recognition detection."
      ],
      "metadata": {
        "id": "a1qb38gtfYOu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Function to calculate accuracy of the LBPH model\n",
        "def calculate_accuracy(model, test_images_path, names):\n",
        "    correct_predictions = 0\n",
        "    total_images = 0\n",
        "\n",
        "    # Walk through the test images directory\n",
        "    for subdir, dirs, files in os.walk(test_images_path):\n",
        "        for filename in files:\n",
        "            # Check if the file is an image (e.g., jpg, png, jpeg) and not '.ipynb_checkpoints'\n",
        "            if filename.lower().endswith(('.jpg', '.png', '.jpeg')):\n",
        "                # Load the image\n",
        "                img_path = os.path.join(subdir, filename)\n",
        "                img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
        "\n",
        "                if img is not None:\n",
        "                    # Resize the image to match the input size of the model\n",
        "                    resized_img = cv2.resize(img, (150, 150))\n",
        "\n",
        "                    # Perform prediction\n",
        "                    label_id, confidence = model.predict(resized_img)\n",
        "\n",
        "                    # Get the true label (assumed to be the folder name)\n",
        "                    true_label = os.path.basename(subdir)\n",
        "\n",
        "                    # Compare the predicted label to the true label\n",
        "                    if names.get(label_id) == true_label:\n",
        "                        correct_predictions += 1\n",
        "\n",
        "                    total_images += 1\n",
        "                else:\n",
        "                    print(f\"Failed to load image: {img_path}\")\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = (correct_predictions / total_images) * 100 if total_images else 0\n",
        "    return accuracy\n",
        "\n",
        "# Example usage\n",
        "test_images_path = \"/content/database\"\n",
        "model = cv2.face.LBPHFaceRecognizer_create()\n",
        "model.read(\"face_recognizer.yml\")  # Load the trained model\n",
        "accuracy = calculate_accuracy(model, test_images_path, names)\n",
        "print(f\"Accuracy: {accuracy:.2f}%\")\n"
      ],
      "metadata": {
        "id": "9nBhF-17uIHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Audio Processing and Transcription**"
      ],
      "metadata": {
        "id": "hGcvtIuBrsht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to get full language name from Whisper's language code\n",
        "def get_language_name(language_code):\n",
        "    language_list = [(code, name) for code, name in whisper.tokenizer.LANGUAGES.items()]\n",
        "    for code, name in language_list:\n",
        "        if code == language_code:\n",
        "            return name\n",
        "    return \"Unknown Language\"\n",
        "\n",
        "# Function to preprocess text for TTS\n",
        "def preprocess_text(text):\n",
        "    text = pre_processors.abbreviations(text)\n",
        "    text = pre_processors.end_of_line(text)\n",
        "    text = pre_processors.tone_marks(text)\n",
        "    return text\n",
        "\n",
        "# Function to tokenize and minimize the text\n",
        "def tokenize_and_minimize(text):\n",
        "    tokenizer = Tokenizer([\n",
        "        tokenizer_cases.tone_marks,\n",
        "        tokenizer_cases.period_comma,\n",
        "        tokenizer_cases.colon,\n",
        "        tokenizer_cases.other_punctuation\n",
        "    ])\n",
        "\n",
        "    tokens = tokenizer.run(text)\n",
        "\n",
        "    minimized_tokens = []\n",
        "    for token in tokens:\n",
        "        if not token.strip():\n",
        "            continue\n",
        "        while len(token) > 100:\n",
        "            split_index = token.rfind(' ', 0, 100)\n",
        "            if split_index == -1:\n",
        "                split_index = 100\n",
        "            minimized_tokens.append(token[:split_index])\n",
        "            token = token[split_index:].lstrip()\n",
        "        minimized_tokens.append(token)\n",
        "\n",
        "    return minimized_tokens\n",
        "\n",
        "# Function to generate TTS and play the audio\n",
        "def generate_and_play_audio(text):\n",
        "    # Preprocess and tokenize the text\n",
        "    processed_text = preprocess_text(text)\n",
        "    tokens = tokenize_and_minimize(processed_text)\n",
        "\n",
        "    # Create an in-memory file to store audio data\n",
        "    fp = io.BytesIO()\n",
        "\n",
        "    # Process each token and generate speech\n",
        "    for token in tokens:\n",
        "        if token.strip():\n",
        "            print(f\"Processing token: '{token}'\")\n",
        "            tts = gTTS(text=token, lang='en', slow=False)\n",
        "            tts.write_to_fp(fp)\n",
        "\n",
        "    fp.seek(0)  # Reset the file pointer to the start\n",
        "\n",
        "    # Convert audio data to base64\n",
        "    audio_base64 = base64.b64encode(fp.read()).decode('utf-8')\n",
        "\n",
        "    # Create JavaScript code to play audio\n",
        "    js_code = f\"\"\"\n",
        "        var audio = new Audio(\"data:audio/wav;base64,{audio_base64}\");\n",
        "        audio.play();\n",
        "    \"\"\"\n",
        "    # Execute JavaScript in Colab (or a browser)\n",
        "    display(Javascript(js_code))\n",
        "\n",
        "# Function to save audio file and transcribe it using Whisper API\n",
        "def save_audio(base64data, count):\n",
        "  try:\n",
        "    audio_data = b64decode(base64data)\n",
        "    audio_filename = f\"recording_{count}.wav\"\n",
        "\n",
        "    with open(audio_filename, 'wb') as f:\n",
        "        f.write(audio_data)\n",
        "\n",
        "# Load the Whisper model\n",
        "    model = whisper.load_model(\"turbo\")\n",
        "\n",
        "    # Transcribe the audio file\n",
        "    result = model.transcribe(audio_filename)\n",
        "    # Extract the detected language\n",
        "    detected_language_code = result[\"language\"]\n",
        "    # Get the full language name\n",
        "    detected_language_name = get_language_name(detected_language_code)\n",
        "\n",
        "    # Extract the original transcription\n",
        "    original_transcription = result[\"text\"]\n",
        "    # Translate the transcription to English\n",
        "    translated_result = model.transcribe(audio_filename, language=\"en\")  # Using English language code\n",
        "    english_translation = translated_result[\"text\"]\n",
        "\n",
        "    # Save the transcription to a text file\n",
        "    transcription_filename = f\"transcription_{count}.txt\"\n",
        "    with open(transcription_filename, 'w') as f:\n",
        "        f.write(result[\"text\"])\n",
        "    # Now, read the content of the transcription file into the 'transcription' variable\n",
        "    with open(transcription_filename, 'r') as f:\n",
        "        transcription = f.read()\n",
        "    # Send the transcription to the text box and the detected language to the inline container\n",
        "    output.eval_js(f\"\"\"\n",
        "        document.getElementById('transcriptionBox').value += 'Original Transcription: \\\\n{original_transcription}\\\\n\\\\nEnglish Translation: \\\\n{english_translation}';\n",
        "        document.getElementById('detectedLanguageBox').innerText = `{detected_language_name}`;\n",
        "    \"\"\")\n",
        "    # Generate and play audio for the transcription\n",
        "    generate_and_play_audio(original_transcription)\n",
        "\n",
        "  except Exception as e:\n",
        "        print(f\"Error in processing audio: {str(e)}\")\n",
        "\n",
        "# Function to save transcription text from the text box\n",
        "def save_text(transcription_text):\n",
        "    try:\n",
        "        # Save the transcription text to a file (append if file exists)\n",
        "        with open(\"transcription.txt\", \"a\") as f:\n",
        "            f.write(transcription_text + '\\n')  # Add a new line after each save\n",
        "        print(\"Text saved successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving text: {str(e)}\")"
      ],
      "metadata": {
        "id": "0k1Q2Xn6DsxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Audio Evaluation"
      ],
      "metadata": {
        "id": "6ErwQFc9LuJs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Speech to Text (STT) system evaluation (Run this code after transcription)"
      ],
      "metadata": {
        "id": "hMDD6Lc5BAq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "def calculate_metrics(reference_text, predicted_text):\n",
        "    \"\"\"\n",
        "    Calculate evaluation metrics for a speech-to-text system.\n",
        "\n",
        "    Args:\n",
        "        reference_text (str): The ground truth text.\n",
        "        predicted_text (str): The transcription output.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary containing accuracy, precision, recall, and F1 score.\n",
        "    \"\"\"\n",
        "    # Tokenize the reference and predicted texts\n",
        "    reference_tokens = reference_text.lower().split()\n",
        "    predicted_tokens = predicted_text.lower().split()\n",
        "\n",
        "    # Align tokens using SequenceMatcher to simulate true positives, false positives, and false negatives\n",
        "    matcher = SequenceMatcher(None, reference_tokens, predicted_tokens)\n",
        "    matches = matcher.get_matching_blocks()\n",
        "\n",
        "    # Calculate true positives, false positives, and false negatives\n",
        "    true_positives = sum(m.size for m in matches[:-1])  # Ignore the last match (dummy match)\n",
        "    false_positives = len(predicted_tokens) - true_positives\n",
        "    false_negatives = len(reference_tokens) - true_positives\n",
        "\n",
        "    # Precision, recall, and F1 score calculations\n",
        "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "    f1 = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    # Accuracy: Proportion of correctly predicted words\n",
        "    accuracy = true_positives / len(reference_tokens) if len(reference_tokens) > 0 else 0\n",
        "\n",
        "    # Return the metrics as a dictionary\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1\n",
        "    }\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "9N_UAW7XATav"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Step 1: Define the reference text (ground truth)\n",
        "        reference_text = \"I love programming and building AI systems.\"\n",
        "\n",
        "        # Step 2: Read the transcription text from the file\n",
        "        transcription_file_path = \"/content/transcription_0.txt\"\n",
        "        with open(transcription_file_path, \"r\") as file:\n",
        "            transcription = file.read().strip()  # Read and strip any extra whitespace\n",
        "\n",
        "        # Step 3: Calculate metrics\n",
        "        metrics = calculate_metrics(reference_text, transcription)\n",
        "\n",
        "        # Step 4: Print the evaluation metrics\n",
        "        print(\"Evaluation Metrics:\")\n",
        "        for metric, value in metrics.items():\n",
        "            print(f\"{metric.capitalize()}: {value:.2f}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to process transcription: {e}\")"
      ],
      "metadata": {
        "id": "90FeKZ2s-klA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text to Speech (TTS) Evaluation"
      ],
      "metadata": {
        "id": "sJqgJ3jTBOwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import whisper\n",
        "\n",
        "# Function to calculate precision, recall, and F1 score\n",
        "def calculate_token_metrics(reference_text, transcription):\n",
        "    \"\"\"\n",
        "    Calculates precision, recall, and F1 score based on token-level comparison.\n",
        "\n",
        "    Args:\n",
        "        reference_text (str): The ground truth transcription.\n",
        "        transcription (str): The generated transcription to compare with the ground truth.\n",
        "\n",
        "    Returns:\n",
        "        dict: Evaluation metrics (precision, recall, F1 score).\n",
        "    \"\"\"\n",
        "    # Split the reference and transcription into tokens (words)\n",
        "    reference_tokens = reference_text.split()\n",
        "    transcription_tokens = transcription.split()\n",
        "\n",
        "    # Calculate true positives (TP), false positives (FP), and false negatives (FN)\n",
        "    true_positives = len(set(reference_tokens).intersection(transcription_tokens))\n",
        "    false_positives = len(set(transcription_tokens) - set(reference_tokens))\n",
        "    false_negatives = len(set(reference_tokens) - set(transcription_tokens))\n",
        "\n",
        "    # Calculate precision, recall, and F1 score\n",
        "    precision = true_positives / (true_positives + false_positives) if (true_positives + false_positives) > 0 else 0\n",
        "    recall = true_positives / (true_positives + false_negatives) if (true_positives + false_negatives) > 0 else 0\n",
        "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "\n",
        "    # Accuracy: Proportion of correctly predicted words\n",
        "    accuracy = true_positives / len(reference_tokens) if len(reference_tokens) > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1_score\n",
        "    }\n",
        "\n",
        "# Step 1: Define the reference text (ground truth)\n",
        "reference_text = \"I love programming and building AI systems.\"\n",
        "\n",
        "# Load the Whisper model\n",
        "model = whisper.load_model(\"base\")\n",
        "\n",
        "# Step 2: Transcribe the audio file\n",
        "result = model.transcribe(\"/content/recording_1.wav\")\n",
        "transcription = result[\"text\"]\n",
        "\n",
        "# Save the transcription to a text file\n",
        "with open(\"transcription_test.txt\", \"w\") as text_file:\n",
        "    text_file.write(transcription)\n",
        "\n",
        "# Step 3: Read the content of the transcription file into the 'transcription' variable\n",
        "with open(\"transcription_test.txt\", 'r') as f:\n",
        "    transcription = f.read()\n",
        "\n",
        "# Step 4: Calculate metrics\n",
        "metrics = calculate_token_metrics(reference_text, transcription)\n",
        "\n",
        "# Step 5: Print the evaluation metrics\n",
        "print(\"Evaluation Metrics:\")\n",
        "for metric, value in metrics.items():\n",
        "    print(f\"{metric.capitalize()}: {value:.2f}\")\n"
      ],
      "metadata": {
        "id": "GavkqteNBQIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Latency calculation for STT"
      ],
      "metadata": {
        "id": "mTLR_X5vJCzz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import whisper\n",
        "\n",
        "def calculate_latency(audio_file_path):\n",
        "    try:\n",
        "        # Load the Whisper model (choose appropriate model size)\n",
        "        model = whisper.load_model(\"medium\")\n",
        "\n",
        "        # Start timer before transcription\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Perform transcription\n",
        "        result = model.transcribe(audio_file_path)\n",
        "\n",
        "        # Stop timer after transcription is completed\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Calculate the latency (time taken for transcription)\n",
        "        latency = end_time - start_time\n",
        "\n",
        "        # Print the result (transcription and latency)\n",
        "        print(f\"Transcription: {result['text']}\")\n",
        "        print(f\"Latency: {latency:.2f} seconds\")\n",
        "\n",
        "        return latency\n",
        "    except Exception as e:\n",
        "        print(f\"Error in transcription: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "audio_file_path = \"/content/recording_0.wav\"  # Replace with your audio file path\n",
        "latency = calculate_latency(audio_file_path)\n",
        "if latency:\n",
        "    print(f\"Transcription completed with latency of {latency:.2f} seconds.\")\n",
        "else:\n",
        "    print(\"Failed to transcribe audio.\")\n"
      ],
      "metadata": {
        "id": "6Zp6lYjkJPBN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Latency calculation for Google TTS"
      ],
      "metadata": {
        "id": "39ZrcgQXNwwn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from gtts import gTTS\n",
        "import os\n",
        "\n",
        "def calculate_gtts_latency(text, lang=\"en\"):\n",
        "    try:\n",
        "        # Start timer before text-to-speech conversion\n",
        "        start_time = time.time()\n",
        "\n",
        "        # Perform text-to-speech conversion using gTTS\n",
        "        tts = gTTS(text=text, lang=\"ur\")\n",
        "\n",
        "        # Save the speech audio to a file\n",
        "        audio_file_path = \"output_audio.mp3\"  # You can specify the file path here\n",
        "        tts.save(audio_file_path)\n",
        "\n",
        "        # Stop timer after the audio file is saved\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Calculate the latency (time taken for text-to-speech conversion)\n",
        "        latency = end_time - start_time\n",
        "\n",
        "        # Print the result (audio file path and latency)\n",
        "        print(f\"Generated speech saved to: {audio_file_path}\")\n",
        "        print(f\"Latency: {latency:.2f} seconds\")\n",
        "\n",
        "        return latency\n",
        "    except Exception as e:\n",
        "        print(f\"Error in text-to-speech conversion: {e}\")\n",
        "        return None\n",
        "\n",
        "# Function to read text from a file\n",
        "def read_text_from_file(file_path):\n",
        "    try:\n",
        "        with open(file_path, 'r') as file:\n",
        "            text = file.read()  # Read the entire file content\n",
        "        return text\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading file: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example usage\n",
        "file_path = \"/content/transcription_0.txt\"  # Path to your text file\n",
        "text = read_text_from_file(file_path)\n",
        "print(text)\n",
        "latency = calculate_gtts_latency(text)\n",
        "if latency:\n",
        "    print(f\"Text-to-speech conversion completed with latency of {latency:.2f} seconds.\")\n",
        "else:\n",
        "    print(\"Failed to convert text to speech.\")\n"
      ],
      "metadata": {
        "id": "upRcxRFwNwIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''from gtts import gTTS\n",
        "from IPython.display import Audio\n",
        "\n",
        "# Read the text from a file\n",
        "with open('/content/transcription.txt', 'r') as file:\n",
        "    text = file.read()\n",
        "\n",
        "# Convert the text to speech\n",
        "tts = gTTS(text, lang=\"ur\")\n",
        "tts.save('1.wav')\n",
        "\n",
        "# Play the generated audio\n",
        "sound_file = '1.wav'\n",
        "Audio(sound_file, autoplay=True)'''"
      ],
      "metadata": {
        "id": "1VuhQ7GskJL8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To see the supported languages by Whisper, use the following code."
      ],
      "metadata": {
        "id": "kteRuzwWgBjU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''import whisper\n",
        "\n",
        "# Get Whisper's supported languages\n",
        "supported_languages = whisper.tokenizer.LANGUAGES\n",
        "for code, name in supported_languages.items():\n",
        "    print(f\"{code}: {name}\")\n",
        "'''"
      ],
      "metadata": {
        "id": "7QqT0eHUxLyY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Display"
      ],
      "metadata": {
        "id": "nr1OPKiQL2mM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import output\n",
        "output.register_callback('save_and_process_image', save_and_process_image)\n",
        "output.register_callback('train_model', train_model)\n",
        "output.register_callback('pred', pred)\n",
        "output.register_callback('notebook.save_audio', save_audio)\n",
        "output.register_callback('notebook.save_text', save_text)"
      ],
      "metadata": {
        "id": "n8M_NRTqhKDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the HTML code in Google Colab\n",
        "display(HTML(html_code))"
      ],
      "metadata": {
        "id": "IymLohcoD7v-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#try this is gtts isn't working\n",
        "#!pip install gTTS sounddevice soundfile numpy"
      ],
      "metadata": {
        "id": "wWkIApEbD4IA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#try installing this if facing issue with sound\n",
        "#!nvidia-smi"
      ],
      "metadata": {
        "id": "27JAN3duQHK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A quick way to check the transcription via whisper api"
      ],
      "metadata": {
        "id": "k2dkDPh9gabs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''import whisper\n",
        "\n",
        "model = whisper.load_model(\"turbo\")\n",
        "result = model.transcribe(\"/content/Audio/that-s-true-japanese.wav\")\n",
        "print(result[\"text\"])'''"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qjhboMtt8mrd",
        "outputId": "617ae068-91f7-4194-daf0-095e69f689da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  checkpoint = torch.load(fp, map_location=device)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "そうですね。\n"
          ]
        }
      ]
    }
  ]
}